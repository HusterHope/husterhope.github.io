---
title: "ISMAR2019D2-4-简要参会记录"
layout: post
categories: 做笔记
tags:
  - Re3D
  - AR
  - MR
  - TelePresence
  - UI
  - ISMAR2019
---

ISMAR 2019会议整理的第二部分，主要是一些感兴趣的Paper和小专题。

<!-- more -->

整个会议的Paper Presentation分为十个部分，分别为：

S1. 跟踪和重建
S2. 建模和渲染
S3. 获取和处理
S4. 空间增强现实和近眼显示
S5. 感知和临场感
S6. 运动系统
S7. 多模态/长时间使用
S8. 协作和娱乐
S9. 选择和文本输入
S10. 训练和学习

根据官方数据，本次会议共收到有效稿件163篇，接收了50篇（录用率30.6%），每篇论文均需要作15分钟的汇报。个人在听会过程中偏向于了解三维重建、虚实融合与交互方面的论文内容，所以主要关注S2、S3、S4和S7。

## 三维重建

三维重建方面，基于多视角RGB图片进行三维重建的过程基本已经定型（Structure from Motion）；结合深度传感器进行端到端三维重建的系统已经能够做到实时采集，在线建模。

本次会议中的代表性论文，更多地是在重建流程基础上的扩展。例如，利用在线建模进行远程呈现（Telepresence）的系统[1]；通过分析光照模型进行纹理后处理的离线重建系统[3]等。[1]采用三段式架构：基于RGB-D的实时三维重建系统+中央服务器处理系统+用以漫游的客户端。该工作的核心贡献在于不损失太多模型主观质量的前提下，将客户端的数量从SOTA的4个扩展到了24个。

随着AR头显设备的成熟，AR辅助三维重建的应用开始涌现，主要体现在采集过程中对用户的引导\[2\]\[6\]。借助这类辅助系统，更多非专业用户能够更快获得更好的重建模型。[2]采集RGB-D数据进行在线重建，在采集过程中加入引导用户操作的提示信息，包括相机与被摄物的距离、相机移动速度、复杂部位的采集时间等，该工作的一个显著缺陷是手持部位出现的采集空洞，以及长时间使用容易引起疲劳。相比之下，[6]的工作过程就更加复杂：首先使用Hololens头显（采集深度信息）生成粗糙模型M，然后对M进行外扩、修复、裁剪，并生成RGB采集所需的拍摄点，用户将手持设备移动到对应标定点后进行拍摄，能够获得更高精度的模型。

## 虚实融合

虚实融合是虚拟现实系统的核心。本次会议中见到了以下几种融合方式：

1. 将实物进行等尺寸虚化（常见于VR应用）

这种虚化可以是完全保留物体原始形态的[7]，也可以在虚化后人为改变其形态[16]。这两个工作均用到训练过的神经网络来保证端到端的实时性。前者利用CNN追踪真实物体的位置和形变，后者利用GAN实时改变食物外观。

2. 将虚拟物渲染到真实环境中

这种方法是较为常见的方法，主观融合效果也较为出色。基本原理是在人眼和真实内容中间叠加一层虚拟内容，这在VR[5]和AR[9]应用中均有涉及。\[5]主要构造了VR环境下的虚实融合，以水下全景视频为背景，加入虚拟光照模拟、水纹模拟和可交互物体；[9]出自浙大CG实验室，目标是构造既易于设计师展示，又易于客户理解的产品原型。借助3D打印的可组装模型作为实体，利用Leap Motion捕捉手势和深度信息，在Hololens中叠加虚拟渲染层。

3. 空间增强现实（SAR）

不同于光学式AR和视频式AR，SAR借助新型激光投影设备，在追踪准确性和实时性方面会优于普通的头显设备\[4]\[10]\[17]。[4]将2D虚拟内容投射到光滑的物体表面，通过追踪用户的视点位置，反向计算显示器需要输出的内容。[10]为本次内容的最佳论文奖提名，“Animated Stickies”这个概念很有意思，将2D虚拟内容映射到无标记物体表面，并实现高速实时追踪（400fps+），看上去就像一个虚拟渲染的图像/视频直接被贴在了真实物体表面一样。

4. 近眼显示（Near-Eye Display）

从显示端出发，偏向于硬件设备的系统搭建\[11]\[12]

## 人机交互

人机交互可以粗略分割为用户输入和机器输出两个部分，现阶段常见的输入和输出方式包括：

* 输入：手柄（VR Controller）、注视点（Gaze）、语音和简单的手势
* 输出：视觉反馈（视）、立体声输出（听）、手柄震动反馈（触）

（题外话：通常意义上说人的感官主要包括视、听、触、嗅、味五个方面，但是嗅和味的模拟现阶段只有很少的团队在探索）

新型输入方式主要是可以利用身体的移动产生更自然的交互：

* 将头部位移也作为交互的输入信息（而不局限于头部的旋转）[8];
* 利用智能硬件捕捉用户足部运动信息，实现大范围虚拟场景漫游[13]；

新型输出方面有一些传达触觉反馈的工作：

* 在VR头显上额外增加传感器，基于面部肌肉触觉和温度反馈[15]

* 将真实物体虚拟化，为手指提供触觉反馈[18]（虚拟键盘输入）。

  值得一提的是，[18]这个工作是微软一个大团队所做，也是本次会议的Best Paper。从系统搭建到若干场景和应用Demo均已成型，有一种未来会形成产品落地的感觉，论文的核心思想是提供一种输入方式的映射。目前的键盘基本是一个按键对应一个字母，但虚拟环境下的映射方式远远不止于此（一图胜千言，该项目的Demo展示视频可在Paper的官网上找到）

除此之外，也有针对交互方式进行大规模用户调研的工作[14]，或提升输入准确性的改进式工作[19]。

总的来说，交互的创新通常需要配合大量用户调研进行验证，而VR和AR的不少相关工作也会利用用户调研来检验易理解程度。

---

最后说设备和平台。

渲染引擎依然是Unity一家独大。

VR设备主要采用HTC Vive Pro，预计Eye系列出来之后会有更多落地场景和研究应用；AR/MR方面主要是Microsoft Hololens，以及Kinect/Leap Motion这类深度信息采集设备作为辅助。微软11月8日官宣Hololens 2已经开始陆续出货（虽然供应商表示国内明年才能由），相比1代产品成熟了很多，所以预计下一周期可能会有一轮突破和革新。



本文内容主要为个人感悟和简要内容整理，不能代表相关论文原作者的观点。

欢迎讨论交流，以及安利更多有意思的项目和文章 ;)

---

参考文献（部分Paper尚未在线公开）

[1] Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client Live Telepresence（S1）

[2] Tangible and Visible 3D Object Reconstruction in Augmented Reality（S2）

[3] Spatially-Varying Diffuse Reflectance Capture Using Irradiance Map Rendering for Image-Based Modeling Applications（S2）

[4] Augmented Environment Mapping for Appearance Editing of Glossy Surfaces.（S2）

[5] Real-Time Mixed Reality Rendering for Underwater 360°Videos.（S2）

[6] AR HMD Guidance for Controlled Hand-Held 3D Acquisition.（S3）（TVCG）

[7] VR Props: An End to End Pipeline for Transporting Real Objects into Virtual and Augmented Environments.（S3）

[8] DepthMove: Leveraging Head Motions in the Depth Dimension to Interact with Virtual Reality Head-Worn Displays. （S3）

[9] VPModel: High-Fidelity Product Simulation in a Virtual-Physical Environment.（S3）（TVCG）

[10] Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane through a Direct Closed-Loop Alignment. （S4）（TVCG）

[11] Towards a Switchable AR/VR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support.（S4）（TVCG）

[12] Varifocal Occlusion-Capable Optical See-through Augmented Reality Display based on Focus-tunable Optics.（S4）（TVCG）

[13] Accurate and Fast Classification of Foot Gestures for Virtual Locomotion.（S6）

[14] Estimation of Rotation Gain Thresholds Considering FOV, Gender, and Distractors.（S6）（TVCG）

[15] Face/On: Multi-Modal Haptic Feedback for Head-Mounted Displays in Virtual Reality.（S7）（TVCG）

[16] DeepTaste: Augmented Reality Gustatory Manipulation with GAN-based Real-time Food-to-Food Translation. （S7）

[17] Conveying spatial awareness cues in xR collaborations.（S8）（TVCG）

[18] ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality.（S9）（TVCG）

[19] Enhanced Geometric Techniques for Point Marking in Model-Free Augmented Reality（S9）