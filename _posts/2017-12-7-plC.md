---
title: "光场相机基本原理"
layout: post
categories: 做笔记
tags:
  - 3D
  - CG
---

在读论文的过程中，偶然看到了有关光场相机的描述，可用于测量场景中物体的深度信息。出于对摄影的一点爱好加上对这个概念的好奇，遂搜集了若干文献和资料。看下来以后只能说弄懂了大概（数学推导部分没有全部验证），为了加深印象以及以后有需要时便于查阅，在此整理若干要点。

<!-- more -->

### 传统相机相关概念

为了完全理解光场相机理论，有必要先梳理一遍传统相机的相关概念。

![](https://github.com/HusterHope/blogimage/raw/master/plC-1.png)

#### 光圈(Aperture)

对于已经制造好的镜头，我们不可能随意改变镜头的直径，但是我们可以通过在镜头内部加入多边形或者圆形，并且**面积可变的孔状光栅来控制镜头通光量**，这个装置就叫做光圈。

相机中的**光圈优先**常写作AP(Aperture Priority)。

#### 焦距(focal length, 常简写为f)

平行光入射时**从透镜光心到光聚集之焦点的距离**。

传统相机中，成像平面处于镜片的焦平面上。

#### F值(F-number)

> 也有文献里写作f-number，为了不和表示焦距的字母f冲突，这里采用大写F。

我们用F值表达光圈大小。

光圈F值=镜头的焦距/光圈有效孔径，即：

$$N=f/D​$$

#### 景深(DoF, Depth of Field)

当相机的镜头对着某一物体聚焦清晰时，在镜头中心所对的位置，垂直镜头轴线的同一平面的点都可以在感光元件上产生相当清晰的图像，在这个平面沿着镜头轴线的前面和后面一定范围的点也可以结成眼睛可以接受的较清晰的像点，把这个平面的前面和后面的所有景物的距离叫做相机的景深。

直观理解如下图：（出处见水印）

![](https://github.com/HusterHope/blogimage/raw/master/plC-3.jpg)

### 光场和传统成像

为了更加准确地表达光场成像的相关原理，我们还是先从基本概念开始。

#### 光场

光场是**表示光辐射分布的函数**，反映了光波动强度与光波分布位置和传播方向之间的映射关系。在几何光学中，光场指的就是**光线强度在空间中的位置和方向分布**。

文字描述可能不太容易理解，先上图。

![](https://github.com/HusterHope/blogimage/raw/master/plC-4.png)

如何完整表达光线信息？光场的定义解决了这个问题。如上图所示，利用两个已知的平行平面，记录光的传播轨迹和强度。$L(u,v,s,t)$表示光场的一个采样点，uv平面的投影点计作$(u,v)$，st平面的投影点计作$(s,t)$，光强计作$L$，上右图描述的是光线到光强的映射（为便于说明，图中将光场简化为二维）。

核心：

光场是个函数，或者说是一种映射关系。该映射由两个平行平面记录的**四维坐标**到该光线的**光照强度**。

#### 传统相机的成像原理

![](https://github.com/HusterHope/blogimage/raw/master/plC-2.jpeg)

如上图所示，简化后的镜头可以看成凸透镜和探测器的组合。

如果我们将光场概念中的两个平面分别替换为镜头平面uv和探测器平面xy，则得到如下成像过程：

![](https://github.com/HusterHope/blogimage/raw/master/plC-5.png)

且探测器平面中点$(x,y)$的光强为：

$$I(x,y) = \int\int L(u,v,x,y)dudv\qquad(1)$$

其中，$(u,v)$为镜头有效面积（与光圈孔径有关）上的坐标。

从上面的计算过程可以看出，传统相机在计算成像时，**只记录了像点及其对应光强的信息，而丢失了光从进入镜头直到像点这个过程中的轨迹**（丢失了uv平面的信息）。

因此，光场相机在传统相机的基础上，额外记录了光线轨迹。

### 光场相机设计

**光场相机将传统相机的探测器平面替换为微透镜阵列，实现对光线轨迹的记录。**[3]

> 该相机的主结构同人眼相似，只是把视网膜换成了昆虫眼（复眼），在拥有足够运算能力的情况下（比如加上电脑），该相机的效果十分独特。

#### 采集原理

![](https://github.com/HusterHope/blogimage/raw/master/plC-12.png)

上图为文献[1]给出的原理图，不过这幅图很不精确。因为单个微透镜的大小通常在微米级，而相机主镜头大小在生活中随处可见，通常是厘米/毫米级。

根据作者举例给出的微透镜焦距、微透镜宽度、采样像素点宽度和间距的数据，可将微透镜后的细节作如下还原（像素宽度9微米，焦距500微米，微透镜宽度125微米，间距36微米）：

![](https://github.com/HusterHope/blogimage/raw/master/plC-18.png)

每个微透镜单元将主镜头孔径成像到探测器上形成一个宏像素，宏像素中每个像元对应主镜头孔径的一个采样（子孔径）。这就实现了光场采集。

![](https://github.com/HusterHope/blogimage/raw/master/plC-14.png)

#### 光学设计		

为了保证充分利用探测器上的像素，且不出现影像重叠。镜头内部结构需要满足一些特定的几何关系。

![](https://github.com/HusterHope/blogimage/raw/master/plC-13.png)

[^注 ]: 上图中，第一种为理想情况下的分布，第二种有像素浪费，第三种有影像重叠。

为了方便描述相关几何关系，将每个微透镜的中心抽象为小孔，并将距离按下图标记。

![](https://github.com/HusterHope/blogimage/raw/master/plC-7.png)

在相机的标准设计中，微透镜阵列和主镜头的距离$a$即为主镜头的焦距，又知透镜宽度$d$、透镜焦距$f$和像素宽度$p$的前提下，推导$b$的值，即微透镜和探测器的距离。过程如下：

由凸透镜的[成像公式](https://baike.baidu.com/item/%E6%88%90%E5%83%8F%E5%85%AC%E5%BC%8F/506912)知，微透镜的焦距$f$满足

$$f = \frac{ab}{a+b}\qquad(2)$$

若充分利用所有像素，即当子图像正好相切时，由相似三角形

$$\frac{b}{d}=\frac{a+b}{D}\qquad(3)$$

且

$$\frac{p}{b}=\frac{D}{a}\qquad(4)$$

利用(3)(4)式消去$a,d$，带入(2)式，得

$$b = \frac{pf}{d}\qquad(5)$$

即得微透镜阵列和探测器平面的距离。

将(5)带入(4)，得

$$\frac{a}{D}=\frac{f}{d}\qquad(6)$$

回顾传统相机相关概念中对*F-数*的定义，知**主透镜和微透镜阵列应满足F数相等**。

在理想情况下，若微透镜单元的数量为$M1\times M2$，每个微透镜所覆盖的像
元数为$N1\times N2$，则探测器中的像元数应为$M1M2\times N1N2$。

### 光场处理

终于来到最核心的功能了，在采集了若干光场信息后，如何灵活地应用这些数据以实现图像还原呢？

> The purpose of capturing the additional two dimensions of data is to allow us to apply ray-tracing techniques to compute synthetic photographs flexibly from the acquired light.
>
> 采集光场数据的目的便是通过光线追踪技术来计算并灵活地合成图像。		

该部分主要内容整理自文献[2]。

#### 视角和光圈变换

在式(1)中，四维光场各变量的取值区间分别由主镜头孔径和微透镜阵列的大小所决定。若限定其$(u,v)$的积分坐标和范围，所得到的图像则为目标经过主镜头某一子孔径范围所成的像，即对应一个成像视角。

$$I(x,y)=\int_{u}^{u+\Delta u} \int_{v}^{v+\Delta v} L(u,v,x,y)\qquad(7) $$

![](https://github.com/HusterHope/blogimage/raw/master/plC-8.png)

在上图所采集的光场数据中，每个微透镜单元后同一位置的像元均是主镜头同一子孔径的投影，由这些像元可共同组成一幅子孔径图像。如下图(a)可以看出，子孔径图像相当于主镜头减小光圈后在与微透镜阵列等效的像元阵列上所成的像。因此，**子孔径图像具有较大的景深范围，但其信噪比也相应降低**。从四维坐标空间中来看，子孔径图像等于光场在方向维度的水平切片(下图b)。


![](https://github.com/HusterHope/blogimage/raw/master/plC-15.png)

#### 数字对焦和数字变焦

在传统的光学成像系统中，对焦（Focus，又称调焦）与变焦（Zoom）一般都采用机械机构的调节方式使离焦模糊目标变得清晰。二者区别在于，对焦改变的是**探测器像面与镜头之间距离，即像距**；而变焦所改变的是**镜头本身的焦距**。从光学角度来看，对焦是将**光场重新投影到新的像平面上**，而变焦则是**改变了光场的传输方向**。在光场成像系统中，四维光场既已能被记录下来，这就允许我们采用数据计算的方式来改变光场的投影平面或传输方向，分别称之为**数字对焦(Digital refocusing)和数字变焦(Digital zooming)**。

本文的原理部分已经基本结束，下面的部分主要是数学推导过程，若不想深究可直接略过本节，直接看后面光场相机的效果图。

##### 数字对焦

**数字对焦是将采集到的光场重新投影到新的像平面上进行积分。**在下图中，$L(u,s)$为采集到的光场，$U$和$S$分别表示主镜头孔径所在平面和微透镜阵列所在平面，两个平面之间的距离为$l$。选择新的对焦平面$S'$，与$U$面之间的距离为$l'$，，令$l'=\alpha l$。$S'$面上所成的像等于$US'$之间光场的积分，即

$$I(s')=\int L'(u,s')du\qquad (8)$$

![](https://github.com/HusterHope/blogimage/raw/master/plC-16.png)

对同一条光线而言，有

$$L(u,s)=L'(u,s')\qquad (9)$$

根据光线与各平面的交点坐标得

$$\frac{s'-u}{l'}\qquad(10)$$

变换后为

$$s = \frac{s'}{\alpha}+u(1-\frac{1}{\alpha})\qquad(11)$$

将其先后带入(9)和(10)中，得

$$I(s')=\int L(u,\frac{s'}{\alpha}+u(1-\frac{1}{\alpha}))du\qquad (12)$$

式(12)即为光场投影到新对焦面上的成像公式。

##### 数字变焦

**数字变焦改变的是镜头的焦距**，因而也改变了**光线通过镜头时的传播方向**。

在下图中，$L(u,s)$为表示采集到的一个光场采样。

![](https://github.com/HusterHope/blogimage/raw/master/plC-9.png)

若将镜头焦距从$F$变为$F'$，光线将改变其方向成为$L'(u,s_x)$，此时仍存在

$$L(u,s)=L'(u,s_x)\qquad (13)$$

对于$S$面上$s$处的像点，假设其对应物面为$S_0$平面，$l_0$和$s_0$分别是其物距和物点在$S_0$上的坐标，可得以下关系式

$$\frac{1}{l_0}+\frac{1}{l}=\frac{1}{F}\qquad (14)$$

$$\frac{s_0}{-l_0} = \frac{s}{l} \qquad(15)$$

变焦以后，该物点的像也发生改变，若新的像面位置为位于$l'$处的$S'$平面，新的像点毕标为$s’$，则也可以得到

$$\frac{1}{l_0}+\frac{1}{l'}=\frac{1}{F'}\qquad (16)$$

$$\frac{s_0}{-l_0} = \frac{s'}{l'} \qquad(17)$$

同时，变焦后改变了方向的光线$L'(u,s_x)$必经过这一新的像点，故有

$$\frac{u-s'}{l'}=\frac{u-s_x}{l}\qquad(18)$$

联立(14)-(18)式，同时令$l'=\beta l$，得

$$s = s_x -u(1-\frac{1}{\beta})\qquad(19)$$

代入(13)，则有

$$L'(u,s_x) = L(u,s_x-u(1-\frac{1}{\beta}))\qquad (20)$$

从而得到变焦后$S$面上形成的图像为

$$I(s_x)=\int L'(u,s_x)du=\int L(u,s_x-u(1-\frac{1}{\beta}))du\qquad(21)$$

数字对焦和数字变焦在本质上都是**将光场沿某个相应的角度进行投影积分**。

### 光场相机成像效果

> 图自文献[1]

![](https://github.com/HusterHope/blogimage/raw/master/plC-10.png)

上图为采用292*292微透镜阵列获得的原始图像，并将局部细节放大。

![](https://github.com/HusterHope/blogimage/raw/master/plC-11.png)

上图为对原始图像进行数字对焦后得到的结果。

![](https://github.com/HusterHope/blogimage/raw/master/plC-17.png)

上图为多图像合成后的全景深图像，即使距离论文发表过去了十多年，这种视觉效果还是很不错的。

相关仿真实验结果可参考资料[4]。

### 结语

还记得开头说过的*光场相机可用于深度计算*吗？

其实那篇论文的作者提出这一说法后，紧接着就否决了：

> Storing this type ofdatais highly challenging as the 7 dimensions introduce large amounts of data.[5]
>
> 这里提到的“7维”数据包括：3维空间数据、二维方向数据、波长数据和时间数据，虽然我们上面分析的光场模型没有这么复杂，但至少也包含了5维数据，因为过大的信息量不适合实时传输，因此舍弃了光场数据而采用几何数据，这部分内容会在后续的reading类文章中描述。

另外，虽然上面讲了光场相机的诸多优点，但其缺点也是很明显的，例如**分辨率不够，设计复杂，以及微透镜和像素点的排布精度要求高**等，这些缺点导致它并没有取代传统相机的市场地位。

为了整理本文的理论知识，先后参考了若干资料（在结尾的链接中给出），最开始甚至连透镜的焦距计算公式都忘了，之后复习了透镜成像原理，回顾了若干摄影知识，为了推导某些细节还回顾了二重积分等知识，虽然后来发现可能跟专业论文关联不是特别密切，但也还算没白折腾。

如果能帮助到更多人理解相关原理，那就再好不过了。

### 参考资料

\[1][Stanford Tech Report - Light Field Photography with a Hand-held Plenoptic Camera](http://www.graphics.stanford.edu/papers/lfcamera/lfcamera-150dpi.pdf)

\[2][周志良 - 光场成像技术研究](http://f.wanfangdata.com.cn/view/%E5%85%89%E5%9C%BA%E6%88%90%E5%83%8F%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6.aspx?ID=Thesis_Y2126317&transaction=%7b%22ExtraData%22%3a%5b%5d%2c%22IsCache%22%3afalse%2c%22Transaction%22%3a%7b%22DateTime%22%3a%22%5c%2fDate(1512632447650%2b0800)%5c%2f%22%2c%22Id%22%3a%2283a73073-6cf9-4204-8614-a84201026598%22%2c%22Memo%22%3anull%2c%22ProductDetail%22%3a%22Thesis_Y2126317%22%2c%22SessionId%22%3a%22c1179bf1-9e13-40dc-82a5-731573276a5b%22%2c%22Signature%22%3a%22DqtvnJbuPrXlTdGAz0WJ5ieIAgmId4VFuuj1IITehz%5c%2fvidGAHAX5L2DQ1T0C4tBq%22%2c%22TransferIn%22%3a%7b%22AccountType%22%3a%22Income%22%2c%22Key%22%3a%22ThesisFulltext%22%7d%2c%22TransferOut%22%3a%7b%22AccountType%22%3a%22GTimeLimit%22%2c%22Key%22%3a%22%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6%22%7d%2c%22Turnover%22%3a30.00000%2c%22User%22%3anull%2c%22UserIP%22%3a%22111.205.230.127%22%7d%2c%22TransferOutAccountsStatus%22%3a%5b%5d%7d)

\[3][光场相机是如何实现的？ - ccccw的回答 - 知乎](https://www.zhihu.com/question/20511442/answer/24066624)

\[4][光场相机是如何实现的？ - 李超的回答 - 知乎](https://www.zhihu.com/question/20511442/answer/26251790)

[5]Rufael Mekuria,Network Streaming and Compression for Mixed Reality Tele-Immersion