---
title: "ISMAR2019D1-OpenARK"
layout: post
categories: 做笔记
tags:
  - Re3D
  - AR
  - MR
  - TelePresence
  - ISMAR2019
---

抱着纯学习的心态注册了[ISMAR 2019](https://www.ismar19.org/newenweb/index.html)，整理所见所想所得。

<!-- more -->

会议日程为10月14日至18日，其中15日至17日为三天主会，首尾两天是几个Workshop和Tutorial。本文先从第一天开始，梳理10月14日下午的Tutorial1，有关AR开源工具包OpenARK。

汇报团队官网：[Berkeley Vivecenter](https://vivecenter.berkeley.edu/courses/openark-ismar-2019-tutorial/)，据说会议结束后会放出slides。

## Section 1 综述

### 出发点

* 一些人做内容，一些人做内容制作工具 -> OpenARK开源框架

> From *Content* to *Content Creation tools*.

* 更自然的交互方式（Natural communication）-> OpenARK 手势交互
* Microsoft Holoportation -> 互空间问题研究

### AR的任务

- 用最少的图像构造三维结构
- 精确的稠密重建
- 模型没有空洞
- 多用户间可分享/可编辑
- 真实的人物构造
- 隐私和标准：例如不同应用下的模型文件共享

### 建模相关问题

* 传统方法：Build rome in a day
  * 优点：硬件广泛（摄像机）/不限范围/噪声模型简单（高斯模型）/保留表面纹理
  * 缺点：计算代价大/依赖光照/依赖纹理特征/稀疏几何结构

>  Video demo: hololens room capture（with depth cameras）

* 双目立体视觉VS单目深度传感器
  * 前者：有使用范围（最近和最远），对光线环境敏感
  * 后者：表现较好

* ToF的关键：
  * 时间计算
  * 双目视差距离计算

* 深度相机的优缺点：
  * 优点：计算简单/不依赖环境光/不依赖纹理/稠密几何结构
  * 缺点：光线发射器耗能更大/Challenge with sunlight(?)/噪声建模复杂/设备成本较高

* RGB-D数据集
  * [Indoor LIDAR-RGBD Scan](http://redwood-data.org/indoor_lidar_rgbd/download.html)(5 models)
  * [Matterport 3D](https://github.com/niessner/Matterport)（90 scenes）
  * [ScanNet](https://github.com/ScanNet/ScanNet)（1500+ scans）
  * [Gibson](http://gibsonenv.stanford.edu/database/)（572 full buildings）
  * [ShapeNet](https://www.shapenet.org/)（50000+ models）
  * [PanoContext](http://panocontext.cs.princeton.edu/)（700 Panoramas）
  * [More..](http://3dvision.princeton.edu/datasets.html)

此外，还有一部分Auto-Encoder+Point Cloud，以及单相机人物建模的内容，主要方法是借助模型的变形，细节待查。

## Section 2 OpenARK Features

这部分讲OpenARK的技术细节偏多，包括：

* 手部跟踪+SLAM
* 平面检测算法
* 手部识别方法
* 针对不同VR/AR设备的差异化渲染和计算

> 例如带有深度传感器的AR设备可以直接生成追踪结果，而Vive Focus这类利用双目视觉进行深度估计的VR设备则用到了边缘计算。

* 回环检测（Loop Closure Detection）
* 相机漂移问题的解决

> Keyframe-based visual–inertial odometry using nonlinear optimization.

现场播放的一些视频Demo比较出彩，更多细节还是看开源项目页吧（[OpenARK](https://github.com/augcog/OpenARK)）。

## Section 3 多用户AR/VR的互空间问题

这部分汇报内容来自Berkeley的一位大三学生，详细探讨了AR/VR中存在的多用户互空间问题（[论文地址](https://arxiv.org/pdf/1910.05998.pdf)）。

这个问题大致可以作如下定义：

几年前[Microsoft Holoportation](https://www.youtube.com/watch?v=7d59O6cfaM0)效果惊艳，但背后仍有不少问题未能解决。**互空间**（mutual space）问题即为其中之一。

我们可以直观地将「互空间」理解为远程虚拟通信的用户能够自由活动和探索公共区域，在这个区域内多用户间的一切动作都能够被完美还原。不过正如上面视频链接里所演示的那样，这样的空间通常需要预先设置和标定。空间大小、用户/房间数量、通信接口（移动电话/头显/电视等）等方面同样存在问题。

如何能够做到多用户空间共享，即互相访问对方的空间，正是其中的核心问题。

Berkeley这个团队沿着数据集构建->交互区域划分->设计最大化互空间的算法->可视化显示的过程逐步分割这一问题，并初步取得了一定效果。

（技术细节还需再读原文）

Allen Y. Yang教授认为，多用户Mutual space问题会有更长远、更细化的发展方向。

---

经过一个下午的浸泡，越发感受到做学术研究是个长期的过程，一些点必须要调研得足够广，研究时也必须挖的足够深，才能挖出一块属于自己的坑。此外，领导和团队也很重要，领导决定前进方向，团队决定前进速度，也就是“在哪挖”和“挖多快”的问题。OpenARK这种规模的项目团队，每个子问题有5-6个学生探索，从框架提出到构造起现在的功能，前后历时三年。

第三部分汇报冲击力很大，特别是对比Berkeley大三学生的research和自己的本科毕设。

一言难尽，道阻且长。