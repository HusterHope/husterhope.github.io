---
title: "Unite Beijing 2018 (3) Unity Everywhere"
layout: post
tags:
  - Unite2018
  - AR
  - VR
  - MR
  - Games
  - Re3D
  - MVS
  - 3DModeling
  - Animation
  - CG
  - Maya
  - Hololens
categories: 做笔记
---

本文为Unite2018会议记录第三篇，主要整理与Unity相关的行业应用。包括小团队游戏开发，汽车行业，场景三维重建，广电直播和动画短片等五个实际应用。

其他几篇相关文章可以在[这个链接](https://leohope.com/tag/#/Unite2018)里找到。

<!-- more -->

## 小团队游戏开发

5月12日上午第二场，相比第一场明显有了更多媒体嘉宾到场。该分享由Hit-point公司的高崎豊先生 & 上村真裕子女士带来，他们分别是《旅行青蛙》游戏项目的总监和主策划。

### 制作团队

Hit-Point公司成立于2007年9月，注册资本约合58万元人民币，目前现有员工26人，可以说完全是彻头彻尾的小规模公司。据高崎豊介绍，公司内部没有明确的部门划分，但有明确的游戏开发团队。比如旅行青蛙团队就是7人，实际开发人员是4人，包括总监、策划、美术、开发各一人。

就是这样一个小团队作品，截止2018年5月，《旅行青蛙》下载量已突破三千八百万次，中国区贡献了其中80%的量（且全部来自iOS平台）。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-1.jpg)

小团队的缺点就是人少，导致资源少，高崎豊提出的解决方案包括：

* 做自己擅长的领域
* 弹性开发

### 游戏本身

* 设计

策划上村真裕子提到，她的个人兴趣是旅行，因此想做一个旅行题材的游戏。同时，希望能将开发者的爱好和用户产生互动。

为什么选择“青蛙”这个名字？上村老师表示自己其实并不喜欢青蛙，但用了谐音取巧，日文中「青蛙」的发音（カエル）和「归家」（帰る）相同，他们希望传达一种游子回家的感觉，让玩家更有亲近感。

* 游戏系统

这部分简单讲了讲行李和旅行的关系、行李内容和去向的关系、放置时间不规律性、给玩家的惊喜等。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-2.jpg)

* 世界观

绘本风格。本身的游戏场景有限，利用青蛙寄回的照片了解外部世界。

将传统游戏中的上帝视角变为探索视角，带给玩家猎奇的喜悦。

### 与Unity的关系

在不复杂的开发背景下，为何使用Unity制作？

高崎豊介绍，他们团队原先都是采用Eclipse或Android Studio先开发Android平台，然后使用Xcode迁移至iOS。

然而这一次，团队中的开发是个刚入职两年的职员，传统方式可能会较为困难，因此选用了Unity。事实证明该游戏的所有模块都能顺利完成，Unity对个人和小企业来说是非常方便的引擎。

最后他们聊了若干关于游戏制作的心得：

> 几年前的「好游戏」：画面精良+大规模+宣传分发
>
> 小规模公司的突围：找到未发掘的市场

## 汽车行业图形保真(VR)

Unity高级解决方案工程师Luc Vo Van为我们介绍了Unity+工业建模软件[PiXYZ](https://www.pixyz-software.com/)+HTC Vive的结合使用方案。

整个系统的框架大致是，PiXYZ负责工业化细模的读取和减面导出，Unity负责解决渲染和VR设备接口，HTC Vive负责显示最终效果和响应用户交互。

主要技术包括PiXYZ和Unity的数据对接，Unity中的LoD设置，以及一个他们团队自己开发的VR交互插件。最后这个插件可以让用户在虚拟世界中通过手柄的一些手势操作，“剥开”汽车的内部结构，精细到任何一颗小螺丝钉（Luc Vo Van表示十分感谢雷克萨斯公司提供的车辆细模）。

浏览效果大致如下所示：

![](https://github.com/HusterHope/blogimage/raw/master/unite3-3.jpg)

![](https://github.com/HusterHope/blogimage/raw/master/unite3-4.jpg)

这个应用也算是Unity+VR+工业的应用了，更多VR/AR/MR相关的技术应用会在下一篇文章中整理。

## 多视图场景三维重建

整个大会中与我目前的工作最最最最最接近的一场报告，于5月13日上午第一场开始，结果我因为堵车不慎迟到了10分钟（

这个汇报更偏向于规范跑完整个三维重建的流程，由Unity大中华区技术总监张黎明老师提供，先后讲述了从不同场景下的数据采集，点云生成和网格化，处理纹理和光照信息，最后放入Unity完成场景渲染，听下来有不少启发，也顺便记了不少小技巧，以后或许可以用上。

团队使用的软件和硬件：

* 软件

![](https://github.com/HusterHope/blogimage/raw/master/unite3-5.jpg)

* 硬件

![](https://github.com/HusterHope/blogimage/raw/master/unite3-6.jpg)

### 图像采集

特定需求的场景建模通常就不会有人提供现有数据集了，因此需要手动采集数据。

对于室外场景来说，应该选取光照正常的无风环境，拍摄时注意季节因素（会影响白天时长/光照角度/植被变化等）

* 视频or图像？

总的来说各有优点。

视频方面，最大的特点是快，并且结合无人机的应用，能够采集大范围和人眼无法直接观测到的场景。但是视频采集最主要的问题是运动模糊，以及对灯光的敏感性。

图像的优点是分辨率高、占用内存小、参数调节方便。这场分享中也主要基于图像采集方法展开。

张老师提供了用单反采集图像时的通用参数表：

> 光圈：f8
>
> 快门：1/160
>
> ISO：100(避免噪点)
>
> 文件格式：RAW

采集后，建议通过直方图初步评价照片质量（分布均衡为好，过于集中为差）

注意控制景深，避免前景模糊。另外，照片的质量比数量更重要。

拍摄时的一些小技巧：

> 相邻照片要有**90%以上**的覆盖区域（嗯这个数值比我想象的高很多
>
> 先纵向拍摄再横向移动
>
> 保持稳定（避免运动模糊
>
> 更高的分辨率需要更近距离的拍摄
>
> 地面数据集：从外向内，循环纹理（避免脚印）

- 纹理一致性

之前没听过这个概念。大意是说：在采集图像时，将场景中置入一张标准色卡，后期根据标准色卡的RGB对图像数值重新调色，从而控制每张图像之间的色差。

* 细节纹理的获取

拍摄两遍：远近各一遍，从轮廓和细节分别对焦。

### 三维重建流程

使用商业软件「[Reality capture](https://www.capturingreality.com/Product)」

需要先把Raw格式的图像转为TIFF：使用DCRaw。

* 距离约束还原

为了还原场景大小，用场景中色卡的已知信息（比如长宽数值）进行缩放。

### 后处理

* 点云存储

对于重建后的点云，观察其效果，若为均匀点云，则用ply格式存储，若为非均匀的点云，则用obj格式存储。由于obj格式不带颜色信息，还需额外的纹理文件保存颜色。

> 通过给出的实例来看，同一个模型，ply格式保存后为7Gb，obj保存后为47Gb

* 网格简化

降低点云密度，降低网格数量。

后期用3ds max手动调整细节瑕疵。

* UV调整
* 烘焙纹理

将高模转为低模+细节，推荐使用Knald。

* 循环纹理的自动生成

https://artomatix.com/

### 去光照

重建场景后，需要先将现实世界中的光照去除，再从游戏场景中加入理想光照。这一步需要首先还原出环境光，再从物体上将其去除，从而保留基本纹理。下图为基本光照模型。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-7.jpg)

对比一下是否去光照对游戏场景的影响，下图中黑色的物体为不去光照的呈现效果。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-8.jpg)

这一步张老师的团队使用Delighting tool（开源）实现。

Q&A环节套瓷到了张老师微信 :D

简单问了问他们团队对于深度相机和多视采集的看法，张老师表示还是要看硬件发展，至少目前来看，高模的重建还是必须用多视采集。此外移动端采集只是精度偏低，还原出大致几何结构问题不大。

但是室内场景人造物体多且杂乱，可能重建过程还是需要结合CV相关的知识进行分割，并加入模型数据库和后期手工编辑，才能有更好的重建效果。

## 广电制播(MR)

这一块的技术融合感觉挺新鲜的，传统的广电领域和新型的混合现实技术相融合，也有了比较系统的解决方案。该专场由北京商询科技公司混合现实技术总监邬浩老师带来。

### 引入

邬老师开场简单提了一下他们对MR的理解：

> 在AR叠加的基础上与真实世界产生交互，MR更适合针对B端（Business）

广电制播中的AR/MR现状：

![](https://github.com/HusterHope/blogimage/raw/master/unite3-9.jpg)

开发需要解决的问题：

> SLAM/空间感知/人机交互（VAMR交互都类似）

对应到Unity中的开发重点：

* Anchor（空间锚）建立是真实和虚拟物体的重要关联。
* Spatial Mapping
* 输入部分

对于广电应用，Unity中还缺乏：

* Anchor调整 
* 跨平台输入交互
* 三维空间中的UI模板
* 多设备协同

因此他们团队开发了[METoolkit](https://github.com/DataMesh-OpenSource/METoolkit)这个中间件。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-10.jpg)

### METoolkit介绍

在讲METoolkit之前，我先理清楚他们是要解决一个什么问题：

我们知道，在广电直播中需要经常切换机位，从多个第三方视角对主体进行拍摄，然而混合现实设备中呈现的场景不便于直接在第三方视角中呈现，特别是在有“推拉摇移”等机位变化的情况下更难以实时定位场景信息。而METoolkit正是为了解决这些问题产生的。

然后邬老师讲了讲METoolKit的功能和架构设计，没有深入看源码，就略过这部分了。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-11.jpg)

![](https://github.com/HusterHope/blogimage/raw/master/unite3-12.jpg)

### 效果展示

（很遗憾没有拍到展示的效果，扒了一下以前的央视新闻找到了当时的录播设备和效果图）

应用：央视诗词大会

![](https://github.com/HusterHope/blogimage/raw/master/unite3-18.jpg)

![](https://github.com/HusterHope/blogimage/raw/master/unite3-19.jpg)

### 后续心得

最后，邬老师分享了若干研发该系统的心得体会。

目前系统容易受网络延迟影响，需要通过设定路由器频段来避开拥塞。

其次，目前Hololens的使用环境还比较有限，特别是在广电演播厅中，很多地方都是反光板和液晶显示频，并带有大面积的黑色环境，导致Hololens难以准确定位。（所以诗词大会的MR直播没有选在演播厅，而是定在了一个书店）

另外，为了正确呈现真实世界物体和虚拟物体的位置关系，需要提前对真实世界的物体范围建粗模。

最后，多设备间的Anchor同步问题建议手工调整（或在场景中放置Marker标定），不建议直接在多设备间发送信息。

顺带给出了他们踩过的若干大坑：

![](https://github.com/HusterHope/blogimage/raw/master/unite3-13.jpg)

Over.

个人非常喜欢这种演讲思路，先把要解决的问题描述清楚，再阐述行业现状和开发需求，接下来提出自己的解决方案并展示效果，最后给听众讲些过程中的坑和心得分享，完全没毛病。让我这个广电和MR的外行也大致理解了他们的工作。

## 动画短片制作

该专场是「中日共同制作Unity酱短片影像的经验分享」，中方主讲人来自苏州舞之动画股份有限公司，日方主讲人是来自Volca株式会社的加治佐興平先生和张中峥先生，前者为导演并制作影片脚本，后者为翻译+中日双方制片协调人。

开讲前先放了原片（8集，每集大概1-2分钟），整体上是木偶剧风格的动作+2D背景/3D前景的分集舞台剧，然而迷之尴尬的中文配音和糟糕的台词，就有不少听众溜了..

理论上今天是会发布片源的，然而截至2018.5.14 19:30，官网还没放出来（http://i.youku.com/i/UNTkxMTE2OTg1Ng==）无所谓啦反正影片本身内容一般般。

开讲后就慢慢有意思了，听的过程差不多是全程欣赏状态，欣赏这种传统手调动画的制作流程。

整个短片由7个中国动画师制作，为了对上嘴型，他们选择了先配音后制作（通常日本动画是先制作后配音的，不考虑嘴型）。

首先提到了Advanced Skeleton5这个Maya骨骼绑定插件，瞬间产生亲切感（差不多一年前用过，真的好用）。

然后讲Unity-Chan（Unity酱）这个人物的模型本身是三角面片的，为了便于刷权重，他们在Maya中重新制作了一个4边形面片的简模。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-14.jpg)

模型和动作OK后，放入Unity布置舞台、灯光、布局机位（固定机位）和2D/3D场景。使用Unity的Timeline剪辑，逐渐迭代。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-15.jpg)

![](https://github.com/HusterHope/blogimage/raw/master/unite3-16.jpg)

用Unity自带的功能做特效和转场，整体效果和用传统动画制作工具的短片没多大差别。还大幅缩短了渲染消耗的时间。

![](https://github.com/HusterHope/blogimage/raw/master/unite3-17.jpg)

Volca公司未来会尝试引入动作捕捉系统，开发二维动画快速制作系统，使用基于移动设备的面部表情捕捉系统（下一篇文章里的iPhone X+ARKit将会介绍）等。

这样看来，新技术的成熟对动画行业的冲击不断，从动作、表情、场景、模型等各个方面都将有新一套解决方案，纯传统工艺制作的动画将越来越少，且看且珍惜吧。

作为三天活动中的最后一场，鼓掌欢送。

---

