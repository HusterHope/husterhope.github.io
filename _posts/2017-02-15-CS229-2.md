---
title: "【CS229-2】监督学习应用与梯度下降"
layout: post
excerpt: "本讲主要涉及的知识点有：线性回归/梯度下降/正规方程组。"
categories: 做笔记
tags:
  - CS229
  - ML
---

本讲主要涉及的知识点有：线性回归/梯度下降/正规方程组。

监督学习的流程: Training set -> Learning algorithm -> h

即: 训练集合 -> 学习算法 -> 假设函数(h)

h的作用为: 接收输入,产生输出.

下面开始详细介绍本讲的内容.

------

## 一.线性回归(Linear Regression)

#### 例1.房屋的价格与面积

首先观察样本数据,并给出一些符号的定义.

样本数据如下图所示,左侧是居住面积(单位:平方英尺),右侧是房屋价格(单位:1000美元).

![](https://github.com/HusterHope/blogimage/raw/master/ML2-1.png)

在本次及以后的课程中,如无特别说明,则规定部分符号含义如下:

m-训练样本个数.在上图的例子中,m=47(图中未完全列出).

x-输入变量/特征

y-输出变量/目标

(x,y)-训练样本        (x(i),y(i))-第i行的训练样本（这里的(i)是上标）

本例中我们假设房屋的价格和面积是线性相关的,因此得到假设函数h(x)=θ0+θ1x.

当训练样本中有两个特征时,h(x)=θ0+θ1×1+θ2×2,例如我们在上表中加入一列:房屋的卧室个数.

![](https://github.com/HusterHope/blogimage/raw/master/ML2-2.png)

推广至一般情形(n个特征):![img](https://github.com/HusterHope/blogimage/raw/master/ML2-3.png)

这里的x和θ都代表一个1行n列的矩阵.

目标：求得θ使下图中的表达式达到最小值.(系数1/2是为了简化之后的计算)![img](https://github.com/HusterHope/blogimage/raw/master/ML2-4.png)

以上即是线性回归的基本思想。

## 二.梯度下降(Gradient Descent)

设想：在一个三维地形图中，从任意一点起，“你环视四周，然后问自己：「如果只迈出一小步，沿哪个方向下山最快？」”

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-5.png)

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-6.png)

由图中结果可以看出：梯度下降的结果有时会依赖参数的初始值。

将以上问题转述为表达式，即

以任意θ作为初始值(不妨设θ=**0(向量)**),不断改变θ使得J(θ)降低.

对θi而言，假设只有一组训练样本，那么:

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-7.png)

于是我们得到了上图Update中所述的学习规则.

其中,α决定了「每一步迈的有多大」.若步子太大,则有可能越过最小值;若步子太小,则收敛时间会变长.

下面运用本性质生成一个算法(将一个训练样本推广至m个),表达式如下图所示.

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-8.png)

我们把这个算法称为批梯度下降算法(Batch Gradient Descent).可以证明的是,由于J(θ)结构的特殊性,这个表达式不会因为初始值的不同而产生不同的收敛结果.

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-9.png)

------

**课堂Q&A**

Q1:为什么接近最小值附近时,「步子」明显变小了?(此现象可观察上图得出)

A:接近局部最小值时,梯度会下降至接近于0,观察表达式可知,每一步会逐渐变小.

Q2:如何判断收敛？

A:根据一些启发式规则,我们可以认为:当迭代前后的两次J(θ)变化极小时,算法就接近结束了.

Q3:这个算法在哪里做到了「环视一周」？

A:事实上,在计算偏导数的时候,我们就获得了梯度下降最快的方向,每一步均是如此.

------

批梯度下降算法的缺陷:每次都要对所有训练样本求和,当训练的数据集很大时,算法效率会大打折扣.

因此提出了一种改进的算法:随机梯度下降(Constant Gradient Descent)

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-10.png)

这种算法的好处显而易见:在每次迭代时,只需重新计算一个参数.

## 三.正规方程组(Normal Equations)

（这一部分基本上是纯数学计算和推导，个人认为方法有些投机，而最关键的几个公式并没有给出推导过程，且我到目前为止也没有弄懂某些定理的推导，所以这部分就当是欣赏一下计算过程吧）

### 1.引入知识

#### （1）矩阵求导

设f是作用于m*n阶矩阵A的函数,并将其映射到实数集.则将f的导数定义为:

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-11.png)

即f关于A的每个元素的偏导数.

#### （2）迹

n*n阶矩阵A,对角线元素之和即为A的迹,记作trA(Trace of A).

### 2.一些定理

(1)trAB=trBA ; trABC=trCAB=trBCA;

(2)trA=tr(A’s Transpose);

(3)若a是实数,则 tra=a ;

(4)![img](https://github.com/HusterHope/blogimage/raw/master/ML2-13.png)

(5)![img](https://github.com/HusterHope/blogimage/raw/master/ML2-14.png)

### 3.推导过程

构造矩阵X如下：

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-15.png)

每行表示一个训练样本.

设**y**为每个训练样本中的结果值,令

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-16.png)

由前文知θ是一个n维向量,则

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-17.png)

稍加变形,我们得到了和前文一样的J(θ).

![img](https://github.com/HusterHope/blogimage/raw/master/ML2-18.png)

为了使J(θ)取得最小,则令∇J(θ)=0.

将∇J(θ)的表达式化简:

![](https://github.com/HusterHope/blogimage/raw/master/ML2-19.png)

说明:

2-3因为括号内的式子运算结果为实数,由定理(3)得变形过程.

3-4因为第二项为实数,所以可直接转置,并与第三项合并,第四项中不含θ,所以直接舍弃.

4-5运用定理(5),设A=θ,B=I(单位向量),C=XTX.

经如上推导，得到正规方程组如下:

![](https://github.com/HusterHope/blogimage/raw/master/ML2-20.png)

解得θ:

![](https://github.com/HusterHope/blogimage/raw/master/ML2-21.png)

------

**课堂Q&A**

Q:XTX会存在不可逆的情况吗？

A:若不可逆,则可能是提供了两组相似(相依赖)的学习特征值,一般会避免这种情况.

------

 

这章的笔记很长,课程视频看的也很慢,目前留下的问题就是那几个有关矩阵的迹和矩阵求导的公式证明了.

后续课程中有解决的问题会加以补充.

更新了四五次之后终于算是把这一章刷过去了,跟数学相关的知识啃起来真不容易…

 

End.