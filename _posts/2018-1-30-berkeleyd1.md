---
title: "2018伯克利冬令营（1）人工智能"
layout: post
categories: 做笔记
tags:
  - AI
  - ML
  - BerkeleyTour
---



北京时间1月28日下午至夜间，我们乘坐的航班在展开的北太平洋上留下一条弧型航线。

<!-- more -->

跨过国际日期变更线，抵达本次冬令营校园所在的城市——旧金山，时间依然是1月28日。加州正午，阳光灿烂。

![](https://github.com/HusterHope/blogimage/raw/master/day1-1.jpeg)

图1 加州俯拍

乘车，吃饭，逛街。带东北口音的随行导游和两顿中国菜，让我们快速适应了这陌生的环境。晚上抵达宾馆，颇想唱上一曲《Hotel California》（加州旅馆）。在拥有冰箱、微波炉、浴缸、阳台、熨衣板的房间里，躺在一米五宽的大床上舒服地早早睡去，以倒时差。

![](https://github.com/HusterHope/blogimage/raw/master/day1-2.jpeg)
图2 真·加州旅馆

---

言归正传，吃饱睡足后，终于在1月29日上午9点，开始了本次行程的第一节课。

第一堂课的主讲人是[许倬](https://www.linkedin.com/in/zhuo-xu-joe/)师兄，本科清华大学，目前在伯克利做机器人相关的研究工作，为我们带来的演讲主题是「机器人的过去，现在和未来」。

> Berkeley Time: 第一堂课之前有个小插曲，我们行程上的约定上课时间为上午9点，但师兄正式开始讲课的时间在9点10分。带队老师说这叫做「Berkeley Time」，因为伯克利的课程安排中间没有课间，而学生下课通常需要赶往另一个教室，因此学校形成习惯，即每堂课都会在整点后过10分钟再正式开始，这种现象被称为伯克利时间（Berkeley Time）。

![](https://github.com/HusterHope/blogimage/raw/master/day1-3.jpeg)

图3 伯克利钟楼

回到课程本身，按师兄的原话说这个题目其实很大，不是很好讲。整堂课听下来觉得也确实如此，内容更多地集中于机器人概念的讨论，机器学习和深度学习相关的知识介绍、强化学习(Reinforcement Learning)在实际科研项目中的应用等。

什么是机器人？不同的人有不同的见解。师兄说，早些时候的设计会让机器人“Shape like human”，即披着金属外壳的机械手臂、人形装置等，后来也让机器人"Think like human"，即让机器学会思考和判断，最典型的应用就像会下围棋的「AlphaGo」。其实，不管是哪一种应用，最重要的理念是“Robots are tools”，即核心都是要为人类服务。

![](https://github.com/HusterHope/blogimage/raw/master/day1-4.jpg)

图4 大众眼中的机器人

接下来是师兄自己的研究方向相关介绍，这部分由机器学习和深度学习作为引入。因为我对大部分概念比较熟悉，听起来不怎么费力。主要内容如下：

1. 机器学习：尝试寻找拟合数据的函数（模型）
2. 回归和分类：优化问题
3. CNN/RNN
4. ImageNet。由于使用了8层神经网络，2012年Hinton等人在图像分类任务挑战中取得了突破性进步。
5. 迁移学习：借用他人的模型，加入自己的数据，做针对性的优化。

![](https://github.com/HusterHope/blogimage/raw/master/day1-5.jpeg)

图5 ImageNet和迁移学习

基础部分引入完成后，师兄开始讲解在机器人方面的应用，主要运用强化学习来完成机器人运动的优化。强化学习首先需要定义回报函数，即机器人作出正确的操作后可以获得奖赏（对应函数值升高），而作出错误操作时获得惩罚（对应函数值降低）。在反复试错和获得奖赏的过程模拟后，机器人便能“学习”出适合应用的运动方法。科研项目的细节内容中有部分是待发表的论文截图，因此不再细说。

![](https://github.com/HusterHope/blogimage/raw/master/day1-6.jpeg)

图6 深度强化学习在自动驾驶中的应用

Q&A部分相对收获更多，因为个人也是直博，因此向师兄请教了博士期间的规划和安排，比如什么时候确定研究点，什么时候开始发表论文，师兄的回答十分中肯：

> 一年级会开始慢慢接触研究领域的核心工作，多与师兄师姐们和导师交流，认清自己。这与自身兴趣和所处环境都很相关
>
> 二年级决定研究方向，论文的产出以质量为重，并和职业规划相关。如果决定去业界就更多做偏工程方面的研究，并积极找实习。
>
> 二年级到四年级一般都是论文的投稿期，论文以质量为重，至少要做出能让自己觉得很不错的成果再考虑发论文。发论文的时间和数量也和导师的性格有关。

基本上这些就是第一堂课的内容了，很佩服师兄在二年级就能够全程英文进行授课和讲解。送自己一句鸡汤：这样的高度不是遥不可及，但需要加倍努力。（附一张许倬师兄的照片([来源：知乎网](https://www.zhihu.com/people/zhuo-xu-3/activities))，听说曾经是清华校草（膜膜膜

![](https://github.com/HusterHope/blogimage/raw/master/day1-7.jpeg)

---

丰盛的午餐过后进入下午的学习。讲师是Doctor [Daniel Aranki](https://people.eecs.berkeley.edu/~daranki/)，课程主题为人工智能（没错就是个这么大的题）

Aranki博士从人工智能的历史讲起，包括最初数学上的悖论、人工智能的两次黄金时期和一次严冬。之后提出问题：什么是人工智能？

一位同学直接给出很标准的回答：将一个自动化的设备放入房间，一个人隔着一堵墙与其对话，当这个人无法分辨房间内是否为真人时，这个设备就能够算是人工智能。


解决一个具体问题的过程可以划分为若干步骤及其决策，让机器找到解决问题的方法就需要用到搜索算法，于是讲了深搜和宽搜(DFS&BFS)，有权图的搜索，启发式搜索，A*寻路等算法，基本上都是大二算法课上的内容，过程简单，不过听英文授课并且跟着想问题还是有点烧脑，当这位doctor一开始问有哪些常见的搜索算法时，我居然只用英文说出了最简单的DFS/BFS，并且其实挺期待他能讲讲AlphaGo里的蒙特卡洛搜索核心机制（然而并没有

搜索讲完了又回到机器学习相关的问题，这部分其实和上午的课程有不少重合，但讲得更为系统，包括经典的定义（可以参考[这篇文章]()），类型（监督学习/无监督学习，漏掉了强化学习，不过上午已经讲过了），数据集划分方法，如何防止过拟合等。

![](https://github.com/HusterHope/blogimage/raw/master/day1-8.jpg)

图7 Dr.Aranki正在回答问题

最后放出必杀：以上讲解内容参考自「[Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)」

下午的课程给我的感觉像是：交互式无字幕科普性公开课。

Aranki神采奕奕、耐心、热情，这是整堂课过后最大的印象。听说日后的行程还会由他带着我们参观Intel公司，期待再会。

---

课程过后参观校园，附几张图片收尾。

![](https://github.com/HusterHope/blogimage/raw/master/day1-10.jpg)
图8 伯克利南楼：已有150余年历史，经历了美国数次大地震依然顽强屹立在校园内，目前为新闻学院所用。

![](https://github.com/HusterHope/blogimage/raw/master/day1-9.jpg)
图9 路边的小松鼠：校园自然环境很好，能时常看见小松鼠，并且不怎么怕人。

![](https://github.com/HusterHope/blogimage/raw/master/day1-12.jpeg)
图10 钟楼前的大图书馆：这里图书馆大楼很多，有的学科还有自己单独的图书馆。



写于2018.1.30

坐标 上图中的图书馆