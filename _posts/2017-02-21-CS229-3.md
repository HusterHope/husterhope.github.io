---
title: "【CS229-3】欠拟合与过拟合"
layout: post
excerpt: "这一节的内容已经有点让人消化不了了..所以准备整理完笔记去把课程Notes打印下来推几遍，过几天再更新后面的内容。"
categories: 做笔记
tags:
  - CS229
  - ML
---

这一节的内容已经有点让人消化不了了..所以准备整理完笔记去把课程Notes打印下来推几遍，过几天再更新后面的内容。

本节要点：

1.欠拟合与过拟合(Underfitting and Overfitting)

2.从线性回归(Linear Regression)到局部加权回归(Locally Weighted Regression)

3.线性模型的概率解释

4.Logistic回归

5.简述：感知器算法

------

## 一、欠拟合与过拟合

（其实吧，看到这节公开课的标题，再听老师讲这一部分的内容，基本上没有什么不懂的，还想着这一讲内容怎么会这么简单？听到后面几部分瞬间打脸了…不过本着先易后难的原则，让我们大致了解一下这部分的概念）

1，回顾

还是以房价预测为例：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-1.png)

可以用线性模型进行预测，得到预测函数：h(x) = θ0+θ1×1 (如上图的直线段)

当我们给出第二个特征：x2=x1^2，则得到了二次模型预测：h(x) = θ0 + θ1×1 +θ2×2 (如上图的曲线段)

可以观察出，在这个模型中，二次模型的预测比线性模型（看上去）要更准确，更加接近数据背后的理论。

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-2.png)

而如果使用更加“精确”的模型来拟合数据，比如上图右下方的高次方程。

但这种拟合有明显的问题：我们采集的数据会受到很多方面因素的影响（比如当天卖家的心情，房价的时间和地区性波动等等），所以单纯构造出一个复杂的拟合曲线并不能很好地预测出结果。

经过上面的分析，我们可以把这个例子下的线形模型预测称为——**欠拟合(Underfitting)**，而把多项式模型预测称为**过拟合(Overfitting)**。

## 二、局部加权回归

### 1.参数学习算法

在正式介绍局部加权回归之前，先引入参数学习算法的概念。

参数学习算法(Parametric learning algorithm)：～是一个参数数量会随着m增长的算法（m为训练集合的大小）。

### 2.局部加权回归

*这个算法不是特别重视特征的选择。

例如，在下图中给出的样本点上，预测任意x下的y值。

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-3.png)

回忆一下线性回归：我们需要先求出θ使J(θ)最小，并返回h(θ)作为预测函数。其结果是，可能得到一条特别“平”的线。

下面引入局部加权回归：我们只考虑x附近固定区域内的点。因此遵循如下步骤：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-4.png)

这里的ω(i)为权值，有多种公式化表达，例如：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-5.png)

这样，如果x(i)接近于x，则ω(i)会约等于1，若x(i)距x很远，则ω(i)会约等于0。

*上面的式子看上去接近正态分布，但是这两者没有关联（但是可以类比理解）

其中，τ被称为bandwidth parameter，译为“波长函数”（其实个人感觉叫参数比较好）。

它控制着权值随距离变远而下降的速率（像是正态分布的方差）。τ较小时，钟形函数较窄；τ较大时函数较宽。

所以，如果沿着x轴对每个点（一定精度下）运用此算法，将能够追踪黑板上的曲线。

------

### 课堂Q&A

*局部加权回归并不能完全避免欠拟合和过拟合的问题。

*τ的确定方法：随模型不同而变化。

*与概率分布函数不同的是：权值函数不一定要积分为1。

*当训练集较大时，该算法效率会大打折扣。

------

## 三、线形回归模型的概率解释(Probabilistic interpretation)

好了从这部分开始就已经开始听的一知半解了，需要重温一些概率论和微积分的知识再来码笔记..

（2.24更新）

在上一节的笔记中，我们提到了使用函数J(θ)来估算误差，并求J(θ)的偏导数，使J(θ)降到最小。

那么不禁要问：为什么选择J(θ)？或者说为什么函数J(θ)不是其他形式？下面就给出这种模型的概率学解释（Probabilistic interpretation）。

首先假设目标变量(y)和输入变量(x)及权重参数(θ)满足下面的等式关系：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-6.png)

其中，ε代表误差项。这里我们假设ε(i)是独立同分布(IID, independently and identically distributed)的，并满足正态分布。

「Q：为什么可以假设ε(i)是IID且满足正态分布？

A：我们得到的每个样本数据都存在一定的误差，且每个样本数据之间没有直接关联（也就是每个房价样本之间没有直接联系）。在大规模统计和概率分析的理论基础上，我们可以认为正态分布是一种较为理想的误差估计模型，并且它便于计算。」

有了上述铺垫，我们就可以把上面的等式改写为如下形式：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-7.png)

这个形式既是误差项的分布，也是(y-θTx)的分布，于是又可以改写为：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-8.png)

则在θ固定时，上式表示给定x的情况下y发生的概率；在我们的例子里，x和y都是固定的（样本数据），所以这个式子就变成了一个关于θ的函数。定义似然函数L(θ)：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-9.png)

因为误差项ε是满足独立同分布(IID)的，所以上式右侧可以用每个给定x(i)下y(i)发生的概率的连乘积来计算，即：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-10.png)

似然函数值的大小意味着每个样本值出现的概率，既然已经出现了m组x、y数据，那么我们有理由认为它们出现的概率比其它值出现的概率大，这就是极大似然估计的简单原理。

所以，为了使我们的假设尽量准确，就要使L(θ)尽量大。为了简化计算，我们引入l(θ)=logL(θ)，并整理得：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-11.png)

所以，为了让L(θ)尽量大，就要让l(θ)尽量大，即使得减号后的式子尽量小：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-12.png)

到这里，我们就真正得到了J(θ)的来源。

> 参考资料：极大似然估计<http://blog.csdn.net/yanqingan/article/details/6125812>

## 四、第一个分类算法：Logistic回归

先看一个例子：设存在标称型目标变量:{0,1}，并满足下图的分布：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-13.png)

这时候我们看出，预测值很大程度上不满足样本的分布情况，所以不能使用线性回归模型来解决问题了，而是转为使用一种新的模型作为预测函数hθ(x)：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-14.png)

这里的g(x)就是**Logistic函数**。

类似地，我们对这个算法也作出概率意义上的解释：

因为y=1或0，所以x，y和θ满足下面的等式：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-15.png)

两式合并整理：（把y=0或y=1带入下式即可重新得到上面的两个方程）

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-16.png)

类似地，定义极大似然函数L(θ)并整理化简：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-17.png)

同样定义l(θ)便于计算：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-18.png)

运用前面学到的梯度下降知识：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-19.png)

注：因为l(θ)和变量θ正相关，所以这里的学习公式中间是正号。

下面推导梯度![img](https://github.com/HusterHope/blogimage/raw/master/ML3-20.png)

于是得到最后的学习公式：

![img](https://github.com/HusterHope/blogimage/raw/master/ML3-21.png)

> 注：这里的公式和前面的线性回归模型的学习公式形式很像，只是h(θ)的定义不同。为什么在不同的模型定义下得到了相同的学习公式呢？其中隐含的理论会在后面的章节中讨论。

------

（感知器算法后续理解再补上）

这一节终于从数学逻辑上整理清楚了。。看来是时候复习一下概率论和线性代数了。
